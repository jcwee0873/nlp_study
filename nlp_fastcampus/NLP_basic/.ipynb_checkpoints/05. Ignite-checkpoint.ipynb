{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f94da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, labels, flatten=True):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.flatten = flatten\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "\n",
    "        if self.flatten:\n",
    "            x = x.view(-1)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def load_mnist(is_train=True, flatten=True):\n",
    "    from torchvision import datasets, transforms\n",
    "\n",
    "    dataset = datasets.MNIST(\n",
    "        '../data', train=is_train, download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ]),\n",
    "    )\n",
    "\n",
    "    x = dataset.data.float() / 255.\n",
    "    y = dataset.targets\n",
    "\n",
    "    if flatten:\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_loaders(config):\n",
    "    x, y = load_mnist(is_train=True, flatten=False)\n",
    "\n",
    "    train_cnt = int(x.size(0) * config.train_ratio)\n",
    "    valid_cnt = x.size(0) - train_cnt\n",
    "\n",
    "    indices = torch.randperm(x.size(0))\n",
    "\n",
    "    train_x, valid_x = torch.index_select(\n",
    "        x,\n",
    "        dim=0,\n",
    "        index=indices\n",
    "    ).split([train_cnt, valid_cnt], dim=0)\n",
    "\n",
    "    train_y, valid_y = torch.index_select(\n",
    "        y,\n",
    "        dim=0,\n",
    "        index=indices\n",
    "    ).split([train_cnt, valid_cnt], dim=0)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=MnistDataset(train_x, train_y, flatten=True),\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=MnistDataset(valid_x, valid_y, flatten=True),\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_x, test_y = load_mnist(is_train=False, flatten=False)\n",
    "    test_loader = DataLoader(\n",
    "        dataset=MnistDataset(test_x, test_y, flatten=True),\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ec82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEngine(Engine):\n",
    "\n",
    "    def __init__(self, func, model, crit, optimizer, config):\n",
    "        self.model = model\n",
    "        self.crit = crit\n",
    "        self.optimizer = optimizer\n",
    "        self.config = config\n",
    "\n",
    "        super().__init__(func)\n",
    "\n",
    "        self.best_loss = np.inf\n",
    "        self.best_model = None\n",
    "\n",
    "        self.device = next(model.parameters()).device\n",
    "\n",
    "    @staticmethod\n",
    "    def train(engine, mini_batch):\n",
    "        engine.model.train()\n",
    "        engine.optimizer.zero_grad()\n",
    "        x, y = mini_batch\n",
    "        x, y = x.to(engine.device), y.to(engine.device)\n",
    "\n",
    "        y_hat = engine.model(x)\n",
    "\n",
    "        loss = engine.crit(y_hat, y)\n",
    "        loss.backward()\n",
    "\n",
    "        # Classification인지 확인\n",
    "        if isinstance(y, torch.LongTensor) or isinstance(y, torch.cuda.Longtensor):\n",
    "            accuracy = (torch.argmax(y_hat, dim=-1) == y).sum() / float(y.size(0))\n",
    "        else :\n",
    "            accuracy = 0\n",
    "\n",
    "        # parameter L2 Norm, Model parameter 학습될수록 높아짐\n",
    "        p_norm = float(get_parameter_norm(engine.model.parameters()))\n",
    "        # Gradient L2 Norm, loss surface의 가파름 정도\n",
    "        g_norm = float(get_grad_norm(engine.model.parameters()))\n",
    "\n",
    "        engine.optimizer.step()\n",
    "\n",
    "        return {\n",
    "            'loss': float(loss),\n",
    "            'accuracy': float(accuracy),\n",
    "            '|param|': p_norm,\n",
    "            '|g_param|': g_norm,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def validate(engine, mini_batch):\n",
    "        engine.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x, y = mini_batch\n",
    "            x, y = x.to(engine.device), y.to(engine.device)\n",
    "\n",
    "            y_hat = engine.model(x)\n",
    "\n",
    "            loss = engine.crit(y_hat, y)\n",
    "\n",
    "            if isinstance(y, torch.LongTensor) or isinstance(y, torch.cuda.Longtensor):\n",
    "                accuracy = (torch.argmax(y_hat, dim=-1) == y).sum() / float(y.size(0))\n",
    "            else :\n",
    "                accuracy = 0\n",
    "\n",
    "        return {\n",
    "            'loss': float(loss),\n",
    "            'accuracy': float(accuracy)\n",
    "        }\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def attach(train_engine, validation_engine, verbose=VERBOSE_BATCH_WISE):\n",
    "\n",
    "        def attach_running_average(engine, metric_name):\n",
    "            RunningAverage(output_transform=lambda x: x[metric_name]).attach(\n",
    "                engine,\n",
    "                metric_name\n",
    "            )\n",
    "\n",
    "        training_metric_names = ['loss', 'accuracy', '|param|', '|g_param|']\n",
    "\n",
    "        for metric_name in training_metric_names:\n",
    "            attach_running_average(train_engine, metric_name)\n",
    "\n",
    "        if verbose >= VERBOSE_BATCH_WISE:\n",
    "            pbar = ProgressBar(bar_format=None, ncols=120)\n",
    "            pbar.attach(train_engine, training_metric_names)\n",
    "\n",
    "        if verbose >= VERBOSE_EPOCH_WISE:\n",
    "            @train_engine.on(Events.EPOCH_COMPLETED)\n",
    "            def print_train_logs(engine):\n",
    "                print('Epoch {} - |param|={:.2e} |g-param|={:.2e} loss={:.4e} accuracy={:.4f}'.format(\n",
    "                    engine.state.epoch,\n",
    "                    engine.state.metrics['|param|'],\n",
    "                    engine.state.metrics['|g_param|'],\n",
    "                    engine.state.metrics['loss'],\n",
    "                    engine.state.metrics['accuracy'],\n",
    "                ))\n",
    "\n",
    "        validation_metric_names = ['loss', 'accuracy']\n",
    "\n",
    "        for metric_name in validation_metric_names:\n",
    "            attach_running_average(validation_engine, metric_name)\n",
    "\n",
    "        if verbose >= VERBOSE_BATCH_WISE:\n",
    "            pbar = ProgressBar(bar_format=None, ncols=120)\n",
    "            pbar.attach(validation_engine, validation_metric_names)\n",
    "\n",
    "        if verbose >= VERBOSE_EPOCH_WISE:\n",
    "            @validation_engine.on(Events.EPOCH_COMPLETED)\n",
    "            def print_validation_logs(engine):\n",
    "                print('Validation - loss={:.4e} accuracy={:.4f} best_loss={:.4e}'.format(\n",
    "                    engine.state.metrics['loss'],\n",
    "                    engine.state.metrics['accuracy'],\n",
    "                    engine.best_loss,\n",
    "                ))\n",
    "\n",
    "    @staticmethod\n",
    "    def check_best(engine):\n",
    "        loss = float(engine.state.metrics['loss'])\n",
    "        if loss <= engine.best_loss:\n",
    "            engine.best_loss = loss\n",
    "            engine.best_model = deepcopy(engine.model.state_dict())\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(engine, train_engine, config, **kwargs):\n",
    "        torch.save({\n",
    "            'model': engine.best_model,\n",
    "            'config': config,\n",
    "            **kwargs\n",
    "        }, config.model_fn)\n",
    "\n",
    "\n",
    "\n",
    "class Trainer_ignite():\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def train(self,\n",
    "              model, crit, optimizer,\n",
    "              train_loader , valid_loader):\n",
    "        train_engine = MyEngine(\n",
    "            MyEngine.train,\n",
    "            model, crit, optimizer, self.config\n",
    "        )\n",
    "        validation_engine = MyEngine(\n",
    "            MyEngine.validate,\n",
    "            model, crit, optimizer, self.config\n",
    "        )\n",
    "\n",
    "        MyEngine.attach(\n",
    "            train_engine,\n",
    "            validation_engine,\n",
    "            verbose=self.config.verbose\n",
    "        )\n",
    "\n",
    "        def run_validation(engine, validation_engine, valid_loader):\n",
    "            validation_engine.run(valid_loader, max_epochs=1)\n",
    "\n",
    "        train_engine.add_event_handler(\n",
    "            Events.EPOCH_COMPLETED,\n",
    "            run_validation,\n",
    "            validation_engine, valid_loader,\n",
    "        )\n",
    "        validation_engine.add_event_handler(\n",
    "            Events.EPOCH_COMPLETED,\n",
    "            MyEngine.check_best,\n",
    "        )\n",
    "        validation_engine.add_event_handler(\n",
    "            Events.EPOCH_COMPLETED,\n",
    "            MyEngine.save_model,\n",
    "            train_engine, self.config\n",
    "        )\n",
    "\n",
    "        train_engine.run(\n",
    "            train_loader,\n",
    "            max_epochs=self.config.n_epochs\n",
    "        )\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5147f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_norm(parameters, norm_type=2):\n",
    "    parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
    "\n",
    "    total_norm = 0\n",
    "\n",
    "    try:\n",
    "        for p in parameters:\n",
    "            param_norm = p.grad.data.norm(norm_type)\n",
    "            total_norm += param_norm ** norm_type\n",
    "        total_norm = total_norm ** (1. / norm_type)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return total_norm\n",
    "\n",
    "def get_parameter_norm(parameters, norm_type=2):\n",
    "    total_norm = 0\n",
    "\n",
    "    try:\n",
    "        for p in parameters:\n",
    "            param_norm = p.data.norm(norm_type)\n",
    "            total_norm += param_norm ** norm_type\n",
    "        total_norm = total_norm ** (1. / norm_type)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a6ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(config):\n",
    "    device = torch.device('cpu') if config.gpu_id < 0 else torch.device('cuda:%d' % config.gpu_id)\n",
    "\n",
    "    train_loader, valid_loader, test_loader = get_loaders(config)\n",
    "    # x, y = load_mnist(is_train=True, flatten=True)\n",
    "    # x, y = split_data(x.to(device), y.to(device), train_ratio=config.train_ratio)\n",
    "\n",
    "    # print(\"Train:\", x[0].shape, y[0].shape)\n",
    "    # print(\"Valid:\", x[1].shape, y[1].shape)\n",
    "\n",
    "    # input_size = int(x[0].shape[-1])\n",
    "    # output_size = int(max(y[0])) + 1\n",
    "\n",
    "    model = ImageClassifier(\n",
    "        input_size=28**2,\n",
    "        output_size=10,\n",
    "        # hidden_sizes=get_hidden_sizes(input_size,\n",
    "        #                               output_size,\n",
    "        #                               config.n_layers),\n",
    "        use_batch_norm=not config.use_dropout,\n",
    "        dropout_p=config.dropout_p,\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    crit = nn.NLLLoss()\n",
    "    # crit = nn.NLLLoss()\n",
    "\n",
    "    if config.verbose >= 1:\n",
    "        print(model)\n",
    "        print(optimizer)\n",
    "        print(crit)\n",
    "\n",
    "    # trainer = Trainer(model, optimizer, crit)\n",
    "\n",
    "    # trainer.train(\n",
    "    #     train_data=(x[0], y[0]), \n",
    "    #     valid_data=(x[1], y[1]), \n",
    "    #     config=config\n",
    "    # )\n",
    "    trainer = Trainer_ignite(config)\n",
    "    trainer.train(model, crit, optimizer, train_loader, valid_loader)\n",
    "\n",
    "    torch.save({\n",
    "        'model': trainer.model.state_dict(),\n",
    "        'opt': optimizer.state_dict(),\n",
    "        'config': config,\n",
    "    }, config.model_fn)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    config = define_argparser()\n",
    "    main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94476f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7713b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
