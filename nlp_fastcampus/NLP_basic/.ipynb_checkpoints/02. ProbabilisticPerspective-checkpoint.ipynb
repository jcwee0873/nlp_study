{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "046924bb",
   "metadata": {},
   "source": [
    "## Statistics Basic  \n",
    "\n",
    "\n",
    "### Discrete Probability Distribution (이산)\n",
    "- Probability Mass Function (확률 질량 함수, PMF)  \n",
    "\n",
    "\n",
    "### Continuous Probability Distribution (연속)\n",
    "- Probability Density Function (확률 밀도 함수, PDF)\n",
    "- Sample에 대한 확률 값을 알 수 없다.  \n",
    "\n",
    "#### 단어는 Discrete한 상태, 그림은 Continuous한 상태?  \n",
    "\n",
    "### Joint Probability, P(x, y)\n",
    "  \n",
    "### Conditional Probability\n",
    "- P(y|x) = P(x,y) / P(x)\n",
    "- P(x,y) = P(y|x) * P(x) = P(x|y) * P(y)  \n",
    "\n",
    "### Bayes Theorem\n",
    "- P(h|D) = P(D|h)P(h) / P(D) (데이터가 주어졌을 때 가설의 확률 = 가설이 주어졌을 때 데이터의 확률)\n",
    "\n",
    "\n",
    "### etc.\n",
    "- P(x) = P(X = x)\n",
    "- P(X)\n",
    "- P(y|x) = P(Y=y | X=x)\n",
    "- P(Y|x) = P(Y | X=x)\n",
    "- P(y|X) = f(x) = P(Y=y | X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623e6db3",
   "metadata": {},
   "source": [
    "### Marginal Distribution\n",
    "- 한 변수를 적분한 형태\n",
    "- P(x) = S P(x,z) dz = S P(x|z)P(z) dz = S P(z|x)P(x) dz = P(x) S P(z|x) dz  \n",
    "\n",
    "### Expectation and Sampling\n",
    "- Ex~P(x) [f(x)] : P(x)라는 분포에서 Sampling한 x를 f(x)에 넣엇을 때의 평균\n",
    "- = Sigma(x -> X) P(x) * f(x)\n",
    "- P(x) = S P(x,z) dz = S P(x|z)P(Z) dz = Ez~P(z)[P(x|z)]\n",
    "- Ex~P(x)[f(x)] := (1/n) * Sigma f(xi), where xi ~ P(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a117b315",
   "metadata": {},
   "source": [
    "### Maximum Likelihood\n",
    "- 소정의 표본에서 Gaussian 분포에서 각 샘플의 길이의 곱이 가장 큰 경우\n",
    "- LikeliHood(u, theta) = ㅠp(xi; u, theta)  \n",
    "\n",
    "#### 입력으로 주어진 확률 분포가 데이터를 얼마나 잘 설명하는지  \n",
    "\n",
    "- Underflow의 가능성으로 Log Likelihood로 문제 해결 \n",
    "- Sigma log P(x=xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f938d7d",
   "metadata": {},
   "source": [
    "### MLE via Gradient Ascent\n",
    "- Gradient Ascent를 통해 파라미터 조정 theta <- theta + a * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b057ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11dd0ed1",
   "metadata": {},
   "source": [
    "## Negative Log Likelihood (NLL)\n",
    "- DNN에서 log liklihood를 최대화하는 parameter 찾기 (-1 곱해서 최소화)\n",
    "- Neural Networks 또한 확률 분포 함수이기 때문에...(W, b)\n",
    "\n",
    "- xi -> DNN -> yi -> softmax == P(yi|xi;theta)\n",
    "\n",
    "#### Minimizing NLL = Minimizing Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11976184",
   "metadata": {},
   "source": [
    "## Maximum A Posterior (MAP)\n",
    "- P(h|D)'posterior' = P(D|h)'likelihood' P(h)'prior' / P(D)'evidence ; h: hypothesis, D: data\n",
    "- likelihood 최대화 -> posterior 최대화\n",
    "\n",
    "#### ex.\n",
    "- 신발사이즈 240일때 범인은 남자? 여자? : P(y|x=240) = P(x=240|y)*P(y) / P(x=240)\n",
    "- 신발 사이즈가 240인 사람이 여자, 남자 중 어디가 많을까 : P(x=240|y)\n",
    "- 범행장소가 군부대라면? : P(y=male) > P(y=female)  \n",
    "\n",
    "### Bayesian vs Frequentist\n",
    "- Bayesian\n",
    "  - Parameter는 random variable이며 prior 분포를 따를것\n",
    "  - 미래의 uncertainty까지 고려\n",
    "- Frequentist\n",
    "  - Parameter는 최적화의 대성\n",
    "  - 현재까지의 정보만을 바탕으로 추정  \n",
    "  \n",
    "#### 아직까지는 Bayesian Deep Learning 결과가 좋지는 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c412ae6a",
   "metadata": {},
   "source": [
    "## Kullback-Leibler Divergence (KL-Divercgence)\n",
    "- 두 분포의 차이를 확인\n",
    "- 각 분포에서 상대 분포상의 차이가 asymmetric\n",
    "- KL(p||q) p관점에서 q와의 차이, -Ex~p(x)[log(q(x)/p(x))] = E[log p/q]\n",
    "- 분포가 비슷할수록 작은 값을 return, 동일분포의 경우 0\n",
    "- 그렇다면 원래 분포, 예측 분포의 KL-Divergence 값이 0이면 최고?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055f811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a3faf8f",
   "metadata": {},
   "source": [
    "## Information and Entropy\n",
    "### Entropy\n",
    "- H(P) = -Ex~p(x)[LogP(x)]\n",
    "- 분포가 flat할수록 Entropy가 높음 (불확실성이 높아짐)\n",
    "- 분포가 sharp할수록 낮음 (불확실성이 낮아짐)  \n",
    "\n",
    "### Cross Entropy\n",
    "- 분포 P에서 바라본 분포 Q의 정보량의 평균\n",
    "- H(P,Q) = -Ex~p(x)[logQ(x)], 분포가 비슷할수록 작은 값을 가짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc83cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4433adcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
