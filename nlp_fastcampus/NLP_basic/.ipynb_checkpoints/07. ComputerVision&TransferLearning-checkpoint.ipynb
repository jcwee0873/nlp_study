{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90dd3644",
   "metadata": {},
   "source": [
    "## Computer Vision\n",
    "- Image Classification\n",
    "  - ImageNet\n",
    "  - Anomaly Detection\n",
    "  - Out of Distributions\n",
    "- Object Detection\n",
    "  - Fast R-CNN\n",
    "  - YOLO\n",
    "- Image Segmentation\n",
    "  - Fully Convolutional Networks(FCN)\n",
    "  - UNet\n",
    "- Imgae Generation\n",
    "  - Generative Models(GAN ... )\n",
    "  - Super Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02efcbb",
   "metadata": {},
   "source": [
    "### VGGNET\n",
    "- 기존 네트워크들이 5X5, 7X7 Conv.layer 이용\n",
    "- 3X3 Conv.layer를 반복사용하여 5X5, 7X7 layer를 대체\n",
    "  - 3X3 * 2 -> 5X5 대체\n",
    "  - 3X3 * 3 -> 7X7 대체\n",
    "- 이를 통해 적은 W로 깊은 네트워크 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab5b96",
   "metadata": {},
   "source": [
    "### ResNET\n",
    "- Residual Connection 이용\n",
    "- ImageNet 대회에서 깊은 네트워크가 우승을 차지\n",
    "  - Gradient Vanishing 및 최적화 문제 발생\n",
    "  - 데이터의 복잡도에 따라서 최적의 깊이도 존재\n",
    "  - 만약 30개의 레이어를 쌓았는데, 20개가 최적이라면? 10개는 y=x 레이어로 만들면 될까? (identity 함수)  \n",
    "  \n",
    "#### Identity Function\n",
    "- F(x) = H(x) - x\n",
    "- H(x) = F(x) + x, F(x) = 0?\n",
    "- => Residual Connection은 Gradient Vanishing을 방지하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960ca115",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "- 각 레이어는 Feature를 추출하는 역할을 함\n",
    "  - conv.layer는 위치에 따라 low-level 또는 high-level feature를 추출\n",
    "  - 데이터가 다르더라도 이미지를 활용한 task에는 공통된 feature이 존재할 것이라 가정\n",
    "- Big Dataset -> Load Weights -> Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a11ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538408da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dataset directory on ./dataset\n",
      "Downloading zip file completed.\n",
      "Unzipping completed.\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from os import listdir, mkdir, rename\n",
    "from os.path import isfile, isdir, join\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "def download_url(url, save_path, chunk_size=128):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)\n",
    "    print('Downloading zip file completed.')\n",
    "\n",
    "\n",
    "def unzip(zip_path, dataset_path):\n",
    "    zf = ZipFile(zip_path)\n",
    "    zf.extractall(path=dataset_path)\n",
    "    zf.close()\n",
    "    print('Unzipping completed.')\n",
    "\n",
    "\n",
    "def restructure_dir(data_path, is_train=True):\n",
    "    files = [f for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "\n",
    "    if is_train:\n",
    "        for file in files:\n",
    "            if not isdir(join(data_path, file.split('.')[0])):\n",
    "                mkdir(join(data_path, file.split('.')[0]))\n",
    "            rename(\n",
    "                join(data_path, file), join(data_path, file.split('.')[0], file)\n",
    "            )\n",
    "    else:\n",
    "        for file in files:\n",
    "            if not isdir(join(data_path, 'dummy')):\n",
    "                mkdir(join(data_path, 'dummy'))\n",
    "            rename(\n",
    "                join(data_path, file), join(data_path, 'dummy', file)\n",
    "            )\n",
    "    print('Resturcturing completed.')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # make dataset directory\n",
    "    dataset_path = './dataset'\n",
    "    if not isdir(dataset_path):\n",
    "        print('Making dataset directory on {}'.format(dataset_path))\n",
    "        mkdir(dataset_path)\n",
    "\n",
    "    # set hymenoptera dataset\n",
    "    hymenoptera_url = 'https://download.pytorch.org/tutorial/hymenoptera_data.zip'\n",
    "    hymenoptera_path = './hymenoptera.zip'\n",
    "\n",
    "    download_url(hymenoptera_url, hymenoptera_path)\n",
    "    unzip(hymenoptera_path, dataset_path)\n",
    "    rename(join(dataset_path, 'hymenoptera_data'), join(dataset_path, 'hymenoptera'))\n",
    "    rename(join(dataset_path, 'hymenoptera', 'val'), join(dataset_path, 'hymenoptera', 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c42a3d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "def load_dataset(\n",
    "    data_transforms,\n",
    "    dataset_dir='./dataset',\n",
    "    dataset_name='catdog',\n",
    "    is_train=True):\n",
    "    dataset_dir = os.path.join(dataset_dir, dataset_name)\n",
    "    is_train_dir = 'train' if is_train else 'test'\n",
    "\n",
    "    dataset = ImageFolder(\n",
    "        os.path.join(dataset_dir, is_train_dir),\n",
    "        data_transforms[is_train_dir]\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def divide_dataset(\n",
    "    train_set,\n",
    "    test_set,\n",
    "    data_name='catdog',\n",
    "    train_ratio=.6,\n",
    "    valid_ratio=.2,\n",
    "    test_ratio=.2):\n",
    "    if data_name == 'catdog':\n",
    "        train_cnt = int(len(train_set) * train_ratio)\n",
    "        valid_cnt = int(len(train_set) * valid_ratio)\n",
    "        test_cnt = len(train_set) - train_cnt - valid_cnt\n",
    "\n",
    "        train_set, valid_set, test_set = random_split(\n",
    "            train_set,\n",
    "            [train_cnt, valid_cnt, test_cnt]\n",
    "        )\n",
    "    elif data_name == 'hymenoptera':\n",
    "        valid_ratio = valid_ratio / (train_ratio + valid_ratio)\n",
    "        valid_cnt = int(len(train_set) * valid_ratio)\n",
    "        train_cnt = len(train_set) - valid_cnt\n",
    "\n",
    "        train_set, valid_set = random_split(\n",
    "            train_set,\n",
    "            [train_cnt, valid_cnt]\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError('You need to specify dataset name.')\n",
    "\n",
    "    return train_set, valid_set, test_set\n",
    "\n",
    "def get_loaders(config, input_size):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(input_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "        ]),\n",
    "    }\n",
    "    \n",
    "    dataset_name = config.dataset_name\n",
    "    train_set = load_dataset(\n",
    "        data_transforms, dataset_name=dataset_name, is_train=True\n",
    "    )\n",
    "    test_set = load_dataset(\n",
    "        data_transforms, dataset_name=dataset_name, is_train=False\n",
    "    )\n",
    "\n",
    "    # Shuffle dataset to split into valid/test set.\n",
    "    train_set, valid_set, test_set = divide_dataset(\n",
    "        train_set, test_set, config.dataset_name,\n",
    "        config.train_ratio, config.valid_ratio, config.test_ratio\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_set,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=valid_set,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_set,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e464741b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
