{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90dd3644",
   "metadata": {},
   "source": [
    "## Computer Vision\n",
    "- Image Classification\n",
    "  - ImageNet\n",
    "  - Anomaly Detection\n",
    "  - Out of Distributions\n",
    "- Object Detection\n",
    "  - Fast R-CNN\n",
    "  - YOLO\n",
    "- Image Segmentation\n",
    "  - Fully Convolutional Networks(FCN)\n",
    "  - UNet\n",
    "- Imgae Generation\n",
    "  - Generative Models(GAN ... )\n",
    "  - Super Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02efcbb",
   "metadata": {},
   "source": [
    "### VGGNET\n",
    "- 기존 네트워크들이 5X5, 7X7 Conv.layer 이용\n",
    "- 3X3 Conv.layer를 반복사용하여 5X5, 7X7 layer를 대체\n",
    "  - 3X3 * 2 -> 5X5 대체\n",
    "  - 3X3 * 3 -> 7X7 대체\n",
    "- 이를 통해 적은 W로 깊은 네트워크 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab5b96",
   "metadata": {},
   "source": [
    "### ResNET\n",
    "- Residual Connection 이용\n",
    "- ImageNet 대회에서 깊은 네트워크가 우승을 차지\n",
    "  - Gradient Vanishing 및 최적화 문제 발생\n",
    "  - 데이터의 복잡도에 따라서 최적의 깊이도 존재\n",
    "  - 만약 30개의 레이어를 쌓았는데, 20개가 최적이라면? 10개는 y=x 레이어로 만들면 될까? (identity 함수)  \n",
    "  \n",
    "#### Identity Function\n",
    "- F(x) = H(x) - x\n",
    "- H(x) = F(x) + x, F(x) = 0?\n",
    "- => Residual Connection은 Gradient Vanishing을 방지하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960ca115",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "- 각 레이어는 Feature를 추출하는 역할을 함\n",
    "  - conv.layer는 위치에 따라 low-level 또는 high-level feature를 추출\n",
    "  - 데이터가 다르더라도 이미지를 활용한 task에는 공통된 feature이 존재할 것이라 가정\n",
    "- Big Dataset -> Load Weights -> Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a11ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538408da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dataset directory on ./dataset\n",
      "Downloading zip file completed.\n",
      "Unzipping completed.\n"
     ]
    }
   ],
   "source": [
    "# import requests \n",
    "# from os import listdir, mkdir, rename\n",
    "# from os.path import isfile, isdir, join\n",
    "# from zipfile import ZipFile\n",
    "\n",
    "\n",
    "# def download_url(url, save_path, chunk_size=128):\n",
    "#     r = requests.get(url, stream=True)\n",
    "#     with open(save_path, 'wb') as fd:\n",
    "#         for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "#             fd.write(chunk)\n",
    "#     print('Downloading zip file completed.')\n",
    "\n",
    "\n",
    "# def unzip(zip_path, dataset_path):\n",
    "#     zf = ZipFile(zip_path)\n",
    "#     zf.extractall(path=dataset_path)\n",
    "#     zf.close()\n",
    "#     print('Unzipping completed.')\n",
    "\n",
    "\n",
    "# def restructure_dir(data_path, is_train=True):\n",
    "#     files = [f for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "\n",
    "#     if is_train:\n",
    "#         for file in files:\n",
    "#             if not isdir(join(data_path, file.split('.')[0])):\n",
    "#                 mkdir(join(data_path, file.split('.')[0]))\n",
    "#             rename(\n",
    "#                 join(data_path, file), join(data_path, file.split('.')[0], file)\n",
    "#             )\n",
    "#     else:\n",
    "#         for file in files:\n",
    "#             if not isdir(join(data_path, 'dummy')):\n",
    "#                 mkdir(join(data_path, 'dummy'))\n",
    "#             rename(\n",
    "#                 join(data_path, file), join(data_path, 'dummy', file)\n",
    "#             )\n",
    "#     print('Resturcturing completed.')\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     # make dataset directory\n",
    "#     dataset_path = './dataset'\n",
    "#     if not isdir(dataset_path):\n",
    "#         print('Making dataset directory on {}'.format(dataset_path))\n",
    "#         mkdir(dataset_path)\n",
    "\n",
    "#     # set hymenoptera dataset\n",
    "#     hymenoptera_url = 'https://download.pytorch.org/tutorial/hymenoptera_data.zip'\n",
    "#     hymenoptera_path = './hymenoptera.zip'\n",
    "\n",
    "#     download_url(hymenoptera_url, hymenoptera_path)\n",
    "#     unzip(hymenoptera_path, dataset_path)\n",
    "#     rename(join(dataset_path, 'hymenoptera_data'), join(dataset_path, 'hymenoptera'))\n",
    "#     rename(join(dataset_path, 'hymenoptera', 'val'), join(dataset_path, 'hymenoptera', 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cb97c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c42a3d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(\n",
    "    data_transforms,\n",
    "    dataset_dir='./dataset',\n",
    "    dataset_name='catdog',\n",
    "    is_train=True):\n",
    "    dataset_dir = os.path.join(dataset_dir, dataset_name)\n",
    "    is_train_dir = 'train' if is_train else 'test'\n",
    "\n",
    "    dataset = ImageFolder(\n",
    "        os.path.join(dataset_dir, is_train_dir),\n",
    "        data_transforms[is_train_dir]\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def divide_dataset(\n",
    "    train_set,\n",
    "    test_set,\n",
    "    data_name='catdog',\n",
    "    train_ratio=.6,\n",
    "    valid_ratio=.2,\n",
    "    test_ratio=.2):\n",
    "    if data_name == 'catdog':\n",
    "        train_cnt = int(len(train_set) * train_ratio)\n",
    "        valid_cnt = int(len(train_set) * valid_ratio)\n",
    "        test_cnt = len(train_set) - train_cnt - valid_cnt\n",
    "\n",
    "        train_set, valid_set, test_set = random_split(\n",
    "            train_set,\n",
    "            [train_cnt, valid_cnt, test_cnt]\n",
    "        )\n",
    "    elif data_name == 'hymenoptera':\n",
    "        valid_ratio = valid_ratio / (train_ratio + valid_ratio)\n",
    "        valid_cnt = int(len(train_set) * valid_ratio)\n",
    "        train_cnt = len(train_set) - valid_cnt\n",
    "\n",
    "        train_set, valid_set = random_split(\n",
    "            train_set,\n",
    "            [train_cnt, valid_cnt]\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError('You need to specify dataset name.')\n",
    "\n",
    "    return train_set, valid_set, test_set\n",
    "\n",
    "def get_loaders(config, input_size):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(input_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "        ]),\n",
    "    }\n",
    "    \n",
    "    dataset_name = config.dataset_name\n",
    "    train_set = load_dataset(\n",
    "        data_transforms, dataset_name=dataset_name, is_train=True\n",
    "    )\n",
    "    test_set = load_dataset(\n",
    "        data_transforms, dataset_name=dataset_name, is_train=False\n",
    "    )\n",
    "\n",
    "    # Shuffle dataset to split into valid/test set.\n",
    "    train_set, valid_set, test_set = divide_dataset(\n",
    "        train_set, test_set, config.dataset_name,\n",
    "        config.train_ratio, config.valid_ratio, config.test_ratio\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_set,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=valid_set,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_set,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec55706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as torch_utils\n",
    "\n",
    "from ignite.engine import Engine\n",
    "from ignite.engine import Events\n",
    "from ignite.metrics import RunningAverage\n",
    "from ignite.contrib.handlers.tqdm_logger import ProgressBar\n",
    "\n",
    "from module_ignite.utils import get_grad_norm, get_parameter_norm\n",
    "\n",
    "\n",
    "VERBOSE_EPOCH_WISE=1\n",
    "VERBOSE_BATCH_WISE=1\n",
    "\n",
    "class MyEngine(Engine):\n",
    "\n",
    "    def __init__(self, func, model, crit, optimizer, config):\n",
    "        self.model = model\n",
    "        self.crit = crit\n",
    "        self.optimizer = optimizer\n",
    "        self.config = config\n",
    "\n",
    "        super().__init__(func)\n",
    "\n",
    "        self.best_loss = np.inf\n",
    "        self.best_model = None\n",
    "\n",
    "        self.device = next(model.parameters()).device\n",
    "\n",
    "    @staticmethod\n",
    "    def train(engine, mini_batch):\n",
    "        engine.model.train()\n",
    "        engine.optimizer.zero_grad()\n",
    "        x, y = mini_batch\n",
    "        x, y = x.to(engine.device), y.to(engine.device)\n",
    "\n",
    "        y_hat = engine.model(x)\n",
    "\n",
    "        loss = engine.crit(y_hat, y)\n",
    "        loss.backward()\n",
    "\n",
    "        # Classification인지 확인\n",
    "        if isinstance(y, torch.LongTensor) or isinstance(y, torch.cuda.LongTensor):\n",
    "            accuracy = (torch.argmax(y_hat, dim=-1) == y).sum() / float(y.size(0))\n",
    "        else :\n",
    "            accuracy = 0\n",
    "\n",
    "        # parameter L2 Norm, Model parameter 학습될수록 높아짐\n",
    "        p_norm = float(get_parameter_norm(engine.model.parameters()))\n",
    "        # Gradient L2 Norm, loss surface의 가파름 정도\n",
    "        g_norm = float(get_grad_norm(engine.model.parameters()))\n",
    "\n",
    "        engine.optimizer.step()\n",
    "\n",
    "        return {\n",
    "            'loss': float(loss),\n",
    "            'accuracy': float(accuracy),\n",
    "            '|param|': p_norm,\n",
    "            '|g_param|': g_norm,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def validate(engine, mini_batch):\n",
    "        engine.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x, y = mini_batch\n",
    "            x, y = x.to(engine.device), y.to(engine.device)\n",
    "\n",
    "            y_hat = engine.model(x)\n",
    "\n",
    "            loss = engine.crit(y_hat, y)\n",
    "\n",
    "            if isinstance(y, torch.LongTensor) or isinstance(y, torch.cuda.LongTensor):\n",
    "                accuracy = (torch.argmax(y_hat, dim=-1) == y).sum() / float(y.size(0))\n",
    "            else :\n",
    "                accuracy = 0\n",
    "\n",
    "        return {\n",
    "            'loss': float(loss),\n",
    "            'accuracy': float(accuracy)\n",
    "        }\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def attach(train_engine, validation_engine, verbose=VERBOSE_BATCH_WISE):\n",
    "\n",
    "        def attach_running_average(engine, metric_name):\n",
    "            RunningAverage(output_transform=lambda x: x[metric_name]).attach(\n",
    "                engine,\n",
    "                metric_name\n",
    "            )\n",
    "\n",
    "        training_metric_names = ['loss', 'accuracy', '|param|', '|g_param|']\n",
    "\n",
    "        for metric_name in training_metric_names:\n",
    "            attach_running_average(train_engine, metric_name)\n",
    "\n",
    "        if verbose >= VERBOSE_BATCH_WISE:\n",
    "            pbar = ProgressBar(bar_format=None, ncols=120)\n",
    "            pbar.attach(train_engine, training_metric_names)\n",
    "\n",
    "        if verbose >= VERBOSE_EPOCH_WISE:\n",
    "            @train_engine.on(Events.EPOCH_COMPLETED)\n",
    "            def print_train_logs(engine):\n",
    "                print('Epoch {} - |param|={:.2e} |g-param|={:.2e} loss={:.4e} accuracy={:.4f}'.format(\n",
    "                    engine.state.epoch,\n",
    "                    engine.state.metrics['|param|'],\n",
    "                    engine.state.metrics['|g_param|'],\n",
    "                    engine.state.metrics['loss'],\n",
    "                    engine.state.metrics['accuracy'],\n",
    "                ))\n",
    "\n",
    "        validation_metric_names = ['loss', 'accuracy']\n",
    "\n",
    "        for metric_name in validation_metric_names:\n",
    "            attach_running_average(validation_engine, metric_name)\n",
    "\n",
    "        if verbose >= VERBOSE_BATCH_WISE:\n",
    "            pbar = ProgressBar(bar_format=None, ncols=120)\n",
    "            pbar.attach(validation_engine, validation_metric_names)\n",
    "\n",
    "        if verbose >= VERBOSE_EPOCH_WISE:\n",
    "            @validation_engine.on(Events.EPOCH_COMPLETED)\n",
    "            def print_validation_logs(engine):\n",
    "                print('Validation - loss={:.4e} accuracy={:.4f} best_loss={:.4e}'.format(\n",
    "                    engine.state.metrics['loss'],\n",
    "                    engine.state.metrics['accuracy'],\n",
    "                    engine.best_loss,\n",
    "                ))\n",
    "\n",
    "    @staticmethod\n",
    "    def check_best(engine):\n",
    "        loss = float(engine.state.metrics['loss'])\n",
    "        if loss <= engine.best_loss:\n",
    "            engine.best_loss = loss\n",
    "            engine.best_model = deepcopy(engine.model.state_dict())\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(engine, train_engine, config, **kwargs):\n",
    "        torch.save({\n",
    "            'model': engine.best_model,\n",
    "            'config': config,\n",
    "            **kwargs\n",
    "        }, config.model_fn)\n",
    "\n",
    "\n",
    "\n",
    "class Trainer_ignite():\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def train(self,\n",
    "              model, crit, optimizer,\n",
    "              train_loader , valid_loader):\n",
    "        train_engine = MyEngine(\n",
    "            MyEngine.train,\n",
    "            model, crit, optimizer, self.config\n",
    "        )\n",
    "        validation_engine = MyEngine(\n",
    "            MyEngine.validate,\n",
    "            model, crit, optimizer, self.config\n",
    "        )\n",
    "\n",
    "        MyEngine.attach(\n",
    "            train_engine,\n",
    "            validation_engine,\n",
    "            verbose=self.config.verbose\n",
    "        )\n",
    "\n",
    "        def run_validation(engine, validation_engine, valid_loader):\n",
    "            validation_engine.run(valid_loader, max_epochs=1)\n",
    "\n",
    "        train_engine.add_event_handler(\n",
    "            Events.EPOCH_COMPLETED,\n",
    "            run_validation,\n",
    "            validation_engine, valid_loader,\n",
    "        )\n",
    "        validation_engine.add_event_handler(\n",
    "            Events.EPOCH_COMPLETED,\n",
    "            MyEngine.check_best,\n",
    "        )\n",
    "        validation_engine.add_event_handler(\n",
    "            Events.EPOCH_COMPLETED,\n",
    "            MyEngine.save_model,\n",
    "            train_engine, self.config\n",
    "        )\n",
    "\n",
    "        train_engine.run(\n",
    "            train_loader,\n",
    "            max_epochs=self.config.n_epochs\n",
    "        )\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9daf5057",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.model_fn = 'model3.pth'\n",
    "        self.gpu_id = 0\n",
    "        self.train_ratio = .6\n",
    "        self.valid_ratio = .2\n",
    "        self.test_ratio = .2\n",
    "        self.batch_size = 256\n",
    "        self.n_epochs = 10\n",
    "        self.verbose = 2\n",
    "        self.model_name = 'resnet'\n",
    "        self.dataset_name = 'catdog'\n",
    "        self.n_classes = 2\n",
    "        # 1. train from scratch (random init)\n",
    "        # 2. train from pretrained weights\n",
    "        # 3. train with freezed pretrained weights\n",
    "        self.freeze = True\n",
    "        self.use_pretrained = True\n",
    "        # <- 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e464741b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Config object at 0x0000018F04F76CD0>\n",
      "Train: 15000\n",
      "Valid: 5000\n",
      "Test: 5000\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "CrossEntropyLoss()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  2%|#5                                                                                          | 1/59 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - |param|=8.80e+01 |g-param|=2.57e+00 loss=4.4048e-01 accuracy=0.7525\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  5%|####6                                                                                       | 1/20 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - loss=1.7578e-01 accuracy=0.9260 best_loss=inf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  2%|#5                                                                                          | 1/59 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - |param|=8.80e+01 |g-param|=8.21e-01 loss=1.6223e-01 accuracy=0.9425\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  5%|####6                                                                                       | 1/20 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - loss=1.4393e-01 accuracy=0.9317 best_loss=1.7578e-01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  2%|#5                                                                                          | 1/59 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - |param|=8.80e+01 |g-param|=9.68e-01 loss=1.4138e-01 accuracy=0.9446\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  5%|####6                                                                                       | 1/20 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - loss=1.2896e-01 accuracy=0.9492 best_loss=1.4393e-01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  2%|#5                                                                                          | 1/59 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - |param|=8.80e+01 |g-param|=6.07e-01 loss=1.2755e-01 accuracy=0.9474\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  5%|####6                                                                                       | 1/20 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - loss=1.3341e-01 accuracy=0.9460 best_loss=1.2896e-01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  2%|#5                                                                                          | 1/59 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - |param|=8.80e+01 |g-param|=7.85e-01 loss=1.2355e-01 accuracy=0.9562\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  5%|####6                                                                                       | 1/20 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - loss=1.1887e-01 accuracy=0.9467 best_loss=1.2896e-01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  2%|#5                                                                                          | 1/59 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - |param|=8.80e+01 |g-param|=8.15e-01 loss=1.1611e-01 accuracy=0.9469\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  5%|####6                                                                                       | 1/20 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - loss=1.0484e-01 accuracy=0.9589 best_loss=1.1887e-01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  2%|#5                                                                                          | 1/59 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - |param|=8.80e+01 |g-param|=5.26e-01 loss=1.1417e-01 accuracy=0.9562\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  5%|####6                                                                                       | 1/20 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - loss=1.1192e-01 accuracy=0.9467 best_loss=1.0484e-01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  2%|#5                                                                                          | 1/59 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - |param|=8.80e+01 |g-param|=7.57e-01 loss=1.2174e-01 accuracy=0.9464\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  5%|####6                                                                                       | 1/20 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - loss=9.8039e-02 accuracy=0.9630 best_loss=1.0484e-01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  2%|#5                                                                                          | 1/59 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - |param|=8.80e+01 |g-param|=4.83e-01 loss=1.1118e-01 accuracy=0.9528\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  5%|####6                                                                                       | 1/20 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - loss=1.1771e-01 accuracy=0.9461 best_loss=9.8039e-02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  2%|#5                                                                                          | 1/59 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - |param|=8.80e+01 |g-param|=6.12e-01 loss=1.0979e-01 accuracy=0.9527\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  5%|####6                                                                                       | 1/20 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - loss=1.0004e-01 accuracy=0.9596 best_loss=9.8039e-02\n"
     ]
    }
   ],
   "source": [
    "def set_parameter_requires_grad(model, freeze):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = not freeze\n",
    "        \n",
    "def main(config):\n",
    "    if config.verbose >= 2:\n",
    "        print(config)\n",
    "    device = torch.device('cpu') if config.gpu_id < 0 else torch.device('cuda:%d' % config.gpu_id)\n",
    "    \n",
    "    model = models.resnet34(pretrained=config.use_pretrained)\n",
    "    set_parameter_requires_grad(model, config.freeze)\n",
    "\n",
    "    n_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(n_features, config.n_classes)\n",
    "    input_size = 224\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_loader, valid_loader, test_loader = get_loaders(config, input_size)\n",
    "    \n",
    "    print(\"Train:\", len(train_loader.dataset))\n",
    "    print(\"Valid:\", len(valid_loader.dataset))\n",
    "    print(\"Test:\", len(test_loader.dataset))\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if config.verbose >= 2:\n",
    "        print(model)\n",
    "        print(optimizer)\n",
    "        print(crit)\n",
    "        \n",
    "    trainer = Trainer_ignite(config)\n",
    "    trainer.train(model, crit, optimizer, train_loader, valid_loader)\n",
    "\n",
    "        \n",
    "config= Config()\n",
    "main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74112949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f0d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
